import logging
from fastapi import HTTPException
from typing import Optional, Dict, Any
import sys
from pathlib import Path

# Add parent directory to path to import services
parent_dir = Path(__file__).parent.parent
if str(parent_dir) not in sys.path:
    sys.path.insert(0, str(parent_dir))
from services.database import get_database

logger = logging.getLogger(__name__)


async def upload_file_to_gridfs(
    file_contents: bytes,
    filename: str,
    content_type: Optional[str] = None,
    description: Optional[str] = None,
    category: Optional[str] = None,
    status: Optional[str] = "pending_classification",
    ai_classified_sensitivity: Optional[str] = None,
    summary: Optional[str] = None
) -> tuple[str, bool]:
    """
    Upload file contents to GridFS. If a file with the same filename exists, it will be updated.
    When updating, this function will:
    - Delete old bounding boxes document
    - Delete old image bounding boxes document
    - Delete old GridFS file and all its chunks
    
    Args:
        file_contents: The file contents as bytes
        filename: The filename (required, cannot be empty)
        content_type: Optional content type
        description: Optional description of the document (user-provided)
        category: Optional category classification
        status: Document status (default: "pending_classification")
        ai_classified_sensitivity: AI classification (default: "unclassified")
        summary: Optional document summary (generated by LLM during parsing)
    
    Returns:
        Tuple of (file_id as string, is_update as bool)
    
    Raises:
        ValueError: If filename is None or empty
    """
    # Validate filename
    if not filename or not filename.strip():
        raise ValueError("Filename is required and cannot be empty")
    
    # Get database and GridFS instance
    logger.debug("Connecting to database")
    db, fs = get_database()
    
    # Check if file with same filename exists
    existing_file = fs.find_one({"filename": filename})
    is_update = existing_file is not None
    
    if is_update:
        logger.info(f"File with filename '{filename}' already exists. Cleaning up old data...")
        old_file_id = existing_file._id
        
        # Delete old bounding boxes document
        try:
            bounding_boxes_collection = db['bounding_boxes']
            delete_result = bounding_boxes_collection.delete_one({'filename': filename})
            if delete_result.deleted_count > 0:
                logger.info(f"Deleted old bounding boxes document for filename: {filename}")
            else:
                logger.debug(f"No bounding boxes document found for filename: {filename}")
        except Exception as e:
            logger.warning(f"Error deleting old bounding boxes: {str(e)}")
            # Continue with file deletion even if bounding boxes deletion fails
        
        # Delete old image bounding boxes document
        try:
            image_boxes_collection = db['bounding_boxes_img']
            delete_result = image_boxes_collection.delete_one({'filename': filename})
            if delete_result.deleted_count > 0:
                logger.info(f"Deleted old image bounding boxes document for filename: {filename}")
            else:
                logger.debug(f"No image bounding boxes document found for filename: {filename}")
        except Exception as e:
            logger.warning(f"Error deleting old image bounding boxes: {str(e)}")
            # Continue with file deletion even if image bounding boxes deletion fails
        
        # Delete the old GridFS file (this should automatically delete from fs.files and all chunks from fs.chunks)
        try:
            if fs.exists(old_file_id):
                fs.delete(old_file_id)
                logger.info(f"Deleted old GridFS file with id: {old_file_id}")
            else:
                logger.warning(f"Old GridFS file with id {old_file_id} does not exist, skipping deletion")
            
            # Verify cleanup: check for orphaned chunks (safety check)
            chunks_collection = db['fs.chunks']
            chunks_count = chunks_collection.count_documents({'files_id': old_file_id})
            if chunks_count > 0:
                logger.warning(f"Found {chunks_count} orphaned chunks for file_id {old_file_id}, cleaning up...")
                chunks_collection.delete_many({'files_id': old_file_id})
                logger.info(f"Deleted {chunks_count} orphaned chunks")
            else:
                logger.debug(f"All chunks for file_id {old_file_id} were properly deleted")
            
            # Verify fs.files entry was deleted (safety check)
            files_collection = db['fs.files']
            files_count = files_collection.count_documents({'_id': old_file_id})
            if files_count > 0:
                logger.warning(f"Found orphaned fs.files entry for file_id {old_file_id}, cleaning up...")
                files_collection.delete_one({'_id': old_file_id})
                logger.info(f"Deleted orphaned fs.files entry")
            else:
                logger.debug(f"fs.files entry for file_id {old_file_id} was properly deleted")
                
        except Exception as e:
            logger.error(f"Error deleting old GridFS file: {str(e)}", exc_info=True)
            # Try to clean up chunks and files manually as fallback
            try:
                chunks_collection = db['fs.chunks']
                chunks_collection.delete_many({'files_id': old_file_id})
                files_collection = db['fs.files']
                files_collection.delete_one({'_id': old_file_id})
                logger.info(f"Manually cleaned up chunks and files for file_id {old_file_id}")
            except Exception as cleanup_error:
                logger.error(f"Error in manual cleanup: {str(cleanup_error)}")
            raise HTTPException(
                status_code=500,
                detail=f"Error deleting old file: {str(e)}"
            )
    
    # Prepare metadata
    metadata = {
        "filename": filename,
        "content_type": content_type or "application/octet-stream"
    }
    
    # Description is optional (user-provided)
    if description:
        metadata["description"] = description
    
    # Summary is optional (generated during parsing)
    if summary:
        metadata["summary"] = summary
    
    if category:
        metadata["category"] = category
    if status:
        metadata["status"] = status
    if ai_classified_sensitivity:
        metadata["ai_classified_sensitivity"] = ai_classified_sensitivity
    
    logger.debug(f"Uploading file to GridFS with metadata: {metadata}")
    # Store file in GridFS
    file_id = fs.put(
        file_contents,
        **metadata
    )
    
    action = "updated" if is_update else "uploaded"
    logger.info(f"File {action} successfully: {filename}, file_id: {file_id}")
    return str(file_id), is_update


def upload_bounding_boxes(
    pdf_file_id: str,
    filename: str,
    extracted_data: Dict[str, Any]
) -> str:
    """
    Upload bounding boxes data to MongoDB collection.
    
    Args:
        pdf_file_id: The ID of the PDF file in GridFS
        filename: The filename of the PDF
        extracted_data: Dictionary containing extracted data with 'pages', 'full_text', 'images' keys
    
    Returns:
        The ID of the inserted bounding boxes document as string
    
    Raises:
        ValueError: If extracted_data is missing required keys
        Exception: For database errors
    """
    logger.debug(f"Uploading bounding boxes for file: {filename}, pdf_file_id: {pdf_file_id}")
    
    try:
        # Validate extracted_data structure
        if not isinstance(extracted_data, dict):
            raise ValueError(f"extracted_data must be a dict, got {type(extracted_data)}")
        
        if 'pages' not in extracted_data:
            raise ValueError("extracted_data is missing 'pages' key")
        if 'full_text' not in extracted_data:
            logger.warning("extracted_data is missing 'full_text' key, using empty string")
            extracted_data['full_text'] = ''
        if 'images' not in extracted_data:
            logger.warning("extracted_data is missing 'images' key, using empty list")
            extracted_data['images'] = []
        
        logger.debug(f"Processing {len(extracted_data['pages'])} pages for bounding boxes")
        
        # Get database instance
        db, _ = get_database()
        logger.debug("Database connection established")
        
        # Prepare bounding boxes data for storage
        pages_data = []
        for page_idx, page_data in enumerate(extracted_data['pages']):
            try:
                page_num = page_data.get('page_number', page_idx + 1)
                
                # Get page text
                text_annotations = page_data.get('text_annotations', [])
                block_texts = [ann['text'] for ann in text_annotations if ann.get('type') == 'block' and ann.get('text')]
                if block_texts:
                    page_text = "\n".join(block_texts)
                else:
                    page_text = "\n".join([ann['text'] for ann in text_annotations if ann.get('text')])
                
                pages_data.append({
                    'page_number': page_num,
                    'text': page_text,
                    'bounding_boxes': text_annotations,
                    'dimensions': page_data.get('dimension', {})
                })
            except Exception as e:
                logger.error(f"Error processing page {page_idx}: {str(e)}", exc_info=True)
                raise
        
        logger.debug(f"Processed {len(pages_data)} pages")
        
        # Prepare bounding boxes document
        bounding_boxes_doc = {
            'pdf_file_id': pdf_file_id,
            'filename': filename,
            'full_text': extracted_data.get('full_text', ''),
            'pages': pages_data,
            'images': extracted_data.get('images', []),
            'summary': {
                'total_pages': len(extracted_data['pages']),
                'total_text_annotations': sum(len(p.get('text_annotations', [])) for p in extracted_data['pages']),
                'total_images': len(extracted_data.get('images', [])),
                'full_text_length': len(extracted_data.get('full_text', ''))
            }
        }
        
        logger.debug("Storing bounding boxes in MongoDB collection")
        # Store bounding boxes in MongoDB collection
        bounding_boxes_collection = db['bounding_boxes']
        
        # Delete any existing bounding boxes for this filename first (should already be deleted by upload_file_to_gridfs, but ensure cleanup)
        delete_result = bounding_boxes_collection.delete_one({'filename': filename})
        if delete_result.deleted_count > 0:
            logger.debug(f"Deleted existing bounding boxes document for filename: {filename}")
        
        # Insert new bounding boxes document
        result = bounding_boxes_collection.insert_one(bounding_boxes_doc)
        if not result.inserted_id:
            raise ValueError(f"Failed to insert bounding boxes document for filename: {filename}")
        
        bounding_boxes_id = str(result.inserted_id)
        logger.info(f"Bounding boxes uploaded successfully for file: {filename}, bounding_boxes_id: {bounding_boxes_id}")
        return bounding_boxes_id
        
    except ValueError as e:
        logger.error(f"Validation error in upload_bounding_boxes: {str(e)}")
        raise
    except Exception as e:
        logger.error(f"Error uploading bounding boxes for {filename}: {str(e)}", exc_info=True)
        logger.error(f"Exception type: {type(e).__name__}")
        raise
